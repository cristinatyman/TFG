{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYoj4t5b2bR2"
      },
      "outputs": [],
      "source": [
        "!pip install youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pysentimiento\n",
        "pip install vader-multi\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "BPXV-blV2_0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RoBERTuito"
      ],
      "metadata": {
        "id": "_nXFZ6ds30bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from pysentimiento import create_analyzer\n",
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
        "\n",
        "# Get the transcript\n",
        "srt = YouTubeTranscriptApi.get_transcript(\"videoID\", languages=['es'])\n",
        "\n",
        "# Combine the text from the transcript segments\n",
        "combined_text = \" \".join([entry['text'] for entry in srt])\n",
        "\n",
        "analyzer.predict(combined_text)"
      ],
      "metadata": {
        "id": "9_QC38vk3263"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VADER"
      ],
      "metadata": {
        "id": "US0luF5M3TpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Get the transcript\n",
        "srt = YouTubeTranscriptApi.get_transcript(\"videoID\", languages=['es'])\n",
        "\n",
        "# Combine the text from the transcript segments\n",
        "combined_text = \" \".join([entry['text'] for entry in srt])\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "analyzer.polarity_scores(combined_text)"
      ],
      "metadata": {
        "id": "01iCmZ3s3SrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For larger captions we analyze it through chunks\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Get the transcript\n",
        "srt = YouTubeTranscriptApi.get_transcript(\"videoID\", languages=['es'])\n",
        "\n",
        "# Combine the text from the transcript segments\n",
        "combined_text = \" \".join([entry['text'] for entry in srt])\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define function to analyze sentiment of each chunk\n",
        "def analyze_sentiment(text_chunk):\n",
        "    return analyzer.polarity_scores(text_chunk)\n",
        "\n",
        "# Define chunk size (number of characters per chunk)\n",
        "chunk_size = 1000\n",
        "\n",
        "# Split the combined text into chunks\n",
        "text_chunks = [combined_text[i:i+chunk_size] for i in range(0, len(combined_text), chunk_size)]\n",
        "\n",
        "# Analyze sentiment for each chunk\n",
        "sentiment_scores = [analyze_sentiment(chunk) for chunk in text_chunks]\n",
        "\n",
        "# Aggregate sentiment scores\n",
        "aggregate_scores = {\n",
        "    'positive': sum(score['pos'] for score in sentiment_scores) / len(sentiment_scores),\n",
        "    'negative': sum(score['neg'] for score in sentiment_scores) / len(sentiment_scores),\n",
        "    'neutral': sum(score['neu'] for score in sentiment_scores) / len(sentiment_scores),\n",
        "    'compound': sum(score['compound'] for score in sentiment_scores) / len(sentiment_scores)\n",
        "}\n",
        "\n",
        "# Print overall sentiment scores\n",
        "print(\"Overall Sentiment Scores:\")\n",
        "print(\"Positive:\", aggregate_scores['positive'])\n",
        "print(\"Negative:\", aggregate_scores['negative'])\n",
        "print(\"Neutral:\", aggregate_scores['neutral'])\n",
        "print(\"Compound:\", aggregate_scores['compound'])"
      ],
      "metadata": {
        "id": "VgHUmyhp3e6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MSPM"
      ],
      "metadata": {
        "id": "46HMKMMk25BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from transformers import pipeline\n",
        "# Create the sentiment classification pipeline\n",
        "classifier = pipeline(\"text-classification\", \"clampert/multilingual-sentiment-covid19\")\n",
        "\n",
        "# Get the transcript\n",
        "srt = YouTubeTranscriptApi.get_transcript(\"videoID\", languages=['es'])\n",
        "\n",
        "# Combine the text from the transcript segments\n",
        "combined_text = \" \".join([entry['text'] for entry in srt])\n",
        "\n",
        "# Analysis\n",
        "result1 = classifier(combined_text)\n",
        "\n",
        "# Get the scores for negative and positive sentiments\n",
        "negative_score = None\n",
        "positive_score = None\n",
        "\n",
        "for res in result1:\n",
        "    if res['label'] == 'negative':\n",
        "        negative_score = res['score']\n",
        "    elif res['label'] == 'positive':\n",
        "        positive_score = res['score']\n",
        "\n",
        "# If any score is None, assign it as the complement of the known score\n",
        "if negative_score is None:\n",
        "    negative_score = 1 - positive_score\n",
        "if positive_score is None:\n",
        "    positive_score = 1 - negative_score\n",
        "\n",
        "# Print the results\n",
        "print([{'label': 'negative', 'score': negative_score}, {'label': 'positive', 'score': positive_score}])"
      ],
      "metadata": {
        "id": "N7lN0Shx2tmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For larger captions we analyze it through chunks\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Create the sentiment classification pipeline\n",
        "classifier = pipeline(\"text-classification\", \"clampert/multilingual-sentiment-covid19\")\n",
        "\n",
        "# Get the transcript\n",
        "srt = YouTubeTranscriptApi.get_transcript(\"videoID\", languages=['es'])\n",
        "\n",
        "# Combine the text from the transcript segments\n",
        "combined_text = \" \".join([entry['text'] for entry in srt])\n",
        "print(\"Combined Text:\\n\", combined_text, \"\\n\")\n",
        "\n",
        "# Function to split text into chunks of a specified max length\n",
        "def chunk_text(text, max_length=512):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    for word in words:\n",
        "        current_length += len(word) + 1  # +1 for the space\n",
        "        if current_length > max_length:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = len(word) + 1\n",
        "        current_chunk.append(word)\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "# Split the combined text into chunks\n",
        "chunks = chunk_text(combined_text)\n",
        "\n",
        "# Classify sentiment for each chunk\n",
        "negative_scores = []\n",
        "positive_scores = []\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    result = classifier(chunk)\n",
        "    chunk_negative_score = None\n",
        "    chunk_positive_score = None\n",
        "\n",
        "    for res in result:\n",
        "        if res['label'] == 'negative':\n",
        "            chunk_negative_score = res['score']\n",
        "        elif res['label'] == 'positive':\n",
        "            chunk_positive_score = res['score']\n",
        "\n",
        "    # If any score is None, assign it as the complement of the known score\n",
        "    if chunk_negative_score is None:\n",
        "        chunk_negative_score = 1 - chunk_positive_score if chunk_positive_score is not None else 0\n",
        "    if chunk_positive_score is None:\n",
        "        chunk_positive_score = 1 - chunk_negative_score if chunk_negative_score is not None else 0\n",
        "\n",
        "    negative_scores.append(chunk_negative_score)\n",
        "    positive_scores.append(chunk_positive_score)\n",
        "\n",
        "    # Print results for each chunk\n",
        "    print(f\"Chunk {i + 1}:\")\n",
        "    print(chunk)\n",
        "    print([\n",
        "        {'label': 'negative', 'score': chunk_negative_score},\n",
        "        {'label': 'positive', 'score': chunk_positive_score}\n",
        "    ])\n",
        "    print()\n",
        "\n",
        "# Calculate average scores\n",
        "average_negative_score = np.mean(negative_scores) if negative_scores else 0\n",
        "average_positive_score = np.mean(positive_scores) if positive_scores else 0\n",
        "\n",
        "# Print the average results\n",
        "print(\"Average Results:\")\n",
        "print([\n",
        "    {'label': 'negative', 'score': average_negative_score},\n",
        "    {'label': 'positive', 'score': average_positive_score}\n",
        "])\n"
      ],
      "metadata": {
        "id": "qLa0y9xc080v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Perspective API"
      ],
      "metadata": {
        "id": "xShCi_0d5LDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from googleapiclient import discovery\n",
        "\n",
        "# Your API key for the Perspective API\n",
        "API_KEY = 'API_KEY'\n",
        "\n",
        "# Get the transcript\n",
        "srt = YouTubeTranscriptApi.get_transcript(\"videoID\", languages=['es'])\n",
        "\n",
        "# Combine the text from the transcript segments\n",
        "combined_text = \" \".join([entry['text'] for entry in srt])\n",
        "print(combined_text)\n",
        "\n",
        "# Initialize the Perspective API client\n",
        "client = discovery.build(\n",
        "    \"commentanalyzer\",\n",
        "    \"v1alpha1\",\n",
        "    developerKey=API_KEY,\n",
        "    discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "    static_discovery=False,\n",
        ")\n",
        "\n",
        "# Function to analyze text and return results\n",
        "def analyze_text(text):\n",
        "    try:\n",
        "        analyze_request = {\n",
        "            'comment': {'text': text},\n",
        "            'requestedAttributes': {\n",
        "                'TOXICITY': {},\n",
        "                'IDENTITY_ATTACK': {},\n",
        "                'INSULT': {},\n",
        "                'PROFANITY': {},\n",
        "                'THREAT': {}\n",
        "            }\n",
        "        }\n",
        "        response = client.comments().analyze(body=analyze_request).execute()\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        error_details = e.error_details[0]\n",
        "        if 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE' in error_details:\n",
        "            print(f\"Skipping analysis for text due to unsupported language: {text}\")\n",
        "        elif 'LANGUAGE_NOT_SUPPORTED' in error_details:\n",
        "            print(f\"Skipping analysis for text due to undefined language: {text}\")\n",
        "        else:\n",
        "            print(f\"Skipping analysis due to error: {error_details}\")\n",
        "        # Return default values of 0 for each category\n",
        "        return {\n",
        "            'attributeScores': {\n",
        "                'TOXICITY': {'summaryScore': {'value': 0}},\n",
        "                'IDENTITY_ATTACK': {'summaryScore': {'value': 0}},\n",
        "                'INSULT': {'summaryScore': {'value': 0}},\n",
        "                'PROFANITY': {'summaryScore': {'value': 0}},\n",
        "                'THREAT': {'summaryScore': {'value': 0}}\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Analyze the combined text\n",
        "response = analyze_text(combined_text)\n",
        "\n",
        "# Print the analysis results\n",
        "result = {}\n",
        "for attribute, scores in response['attributeScores'].items():\n",
        "    score_value = scores['summaryScore']['value']\n",
        "    result[attribute] = score_value\n",
        "    # Extract spanScores values if present\n",
        "    if 'spanScores' in scores:\n",
        "        for score in scores['spanScores']:\n",
        "            score_type = score['score']['type']\n",
        "            score_value = score['score']['value']\n",
        "            result[f'{attribute}_{score_type}'] = score_value\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "XeMiKE1M5KAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For larger captions we analyze it through chunks\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from googleapiclient import discovery\n",
        "\n",
        "# Your API key for the Perspective API\n",
        "API_KEY = 'API_KEY'\n",
        "\n",
        "# Get the transcript\n",
        "srt = YouTubeTranscriptApi.get_transcript(\"videoID\", languages=['es'])\n",
        "\n",
        "# Combine the text from the transcript segments\n",
        "combined_text = \" \".join([entry['text'] for entry in srt])\n",
        "print(combined_text)\n",
        "\n",
        "# Initialize the Perspective API client\n",
        "client = discovery.build(\n",
        "    \"commentanalyzer\",\n",
        "    \"v1alpha1\",\n",
        "    developerKey=API_KEY,\n",
        "    discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "    static_discovery=False,\n",
        ")\n",
        "\n",
        "# Function to analyze text and return results\n",
        "def analyze_text(text):\n",
        "    try:\n",
        "        analyze_request = {\n",
        "            'comment': {'text': text},\n",
        "            'requestedAttributes': {\n",
        "                'TOXICITY': {},\n",
        "                'IDENTITY_ATTACK': {},\n",
        "                'INSULT': {},\n",
        "                'PROFANITY': {},\n",
        "                'THREAT': {}\n",
        "            }\n",
        "        }\n",
        "        response = client.comments().analyze(body=analyze_request).execute()\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        error_details = e.error_details[0]\n",
        "        if 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE' in error_details:\n",
        "            print(f\"Skipping analysis for text due to unsupported language: {text}\")\n",
        "        elif 'LANGUAGE_NOT_SUPPORTED' in error_details:\n",
        "            print(f\"Skipping analysis for text due to undefined language: {text}\")\n",
        "        else:\n",
        "            print(f\"Skipping analysis due to error: {error_details}\")\n",
        "        # Return default values of 0 for each category\n",
        "        return {\n",
        "            'attributeScores': {\n",
        "                'TOXICITY': {'summaryScore': {'value': 0}},\n",
        "                'IDENTITY_ATTACK': {'summaryScore': {'value': 0}},\n",
        "                'INSULT': {'summaryScore': {'value': 0}},\n",
        "                'PROFANITY': {'summaryScore': {'value': 0}},\n",
        "                'THREAT': {'summaryScore': {'value': 0}}\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Function to split text into chunks\n",
        "def split_text(text, max_length):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    chunk = []\n",
        "    chunk_length = 0\n",
        "    for word in words:\n",
        "        if chunk_length + len(word) + 1 > max_length:\n",
        "            chunks.append(' '.join(chunk))\n",
        "            chunk = []\n",
        "            chunk_length = 0\n",
        "        chunk.append(word)\n",
        "        chunk_length += len(word) + 1\n",
        "    if chunk:\n",
        "        chunks.append(' '.join(chunk))\n",
        "    return chunks\n",
        "\n",
        "# Analyze the combined text in chunks\n",
        "max_chunk_length = 5000  # Maximum length of text for Perspective API\n",
        "text_chunks = split_text(combined_text, max_chunk_length)\n",
        "\n",
        "# Store the results of each chunk\n",
        "chunk_results = []\n",
        "\n",
        "for chunk in text_chunks:\n",
        "    response = analyze_text(chunk)\n",
        "    chunk_results.append(response)\n",
        "\n",
        "# Aggregate the results\n",
        "def aggregate_results(chunk_results):\n",
        "    aggregated_result = {}\n",
        "    attribute_sums = {\n",
        "        'TOXICITY': 0,\n",
        "        'IDENTITY_ATTACK': 0,\n",
        "        'INSULT': 0,\n",
        "        'PROFANITY': 0,\n",
        "        'THREAT': 0\n",
        "    }\n",
        "    count = len(chunk_results)\n",
        "    for response in chunk_results:\n",
        "        for attribute, scores in response['attributeScores'].items():\n",
        "            attribute_sums[attribute] += scores['summaryScore']['value']\n",
        "    for attribute, total in attribute_sums.items():\n",
        "        aggregated_result[attribute] = total / count\n",
        "    return aggregated_result\n",
        "\n",
        "result = aggregate_results(chunk_results)\n",
        "\n",
        "# Print the aggregated results\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "Rv_ZezBt5ZVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}